{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "<img src=\"./img/5_classification_vs_regression_graph.png\" width=\"700px\"><br><br>\n",
    "\n",
    "With `Regression` we tried to find the hyperplane that __best fits__ (represents) the data.\n",
    "\n",
    "With `Classification` we want to find the hyperplane that __best separates__ data into similar categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Classifier (k-nearest Neighbours)\n",
    "\n",
    "##### Assumption:\n",
    "\n",
    "> <br>  \n",
    "> Adjacent nodes are similar<br><br>  \n",
    "> <b>\"distance\"</b> can be computed\n",
    "> <br><br>\n",
    "\n",
    "<br><br>\n",
    "<img src=\"./img/5_kNN.png\">\n",
    "\n",
    "<span style=\"font-size: 70%\">Given a labled dataset, a new data point needs to be classified (<b>top left</b>). The distance to a predefined <br>number of neighbours is calculated and the class assigned according to the sum of distances <br>of each class (<b>top right</b>). Only neighbours in a respective radius are considered (<b>bottom</b>).</span>\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/5_kNN_3.png\" width=\"300px\">\n",
    "\n",
    "<span style=\"font-size: 70%\"><b>Note:</b> The value of $k$ determines the class. Changing $k$ may change the class a data point is voted to.<br>In this example a value of $k = 3$ makes the green dot belong to the class of red triangles (<b>inner circle</b>), <br>$k = 5$ assigns the class of blue squares (<b>outer dotted circle</b>).</span>\n",
    "\n",
    "##### Calculating distances:\n",
    "\n",
    "Given data of order $p$:\n",
    "\n",
    "$X = \\left ( x_1, x_2, \\dots , x_n \\right)$ and $Y = \\left ( y_1, y_2, \\dots , y_n \\right) \\epsilon \\  \\mathbb{R}^2$\n",
    "\n",
    "<br>\n",
    "\n",
    "the `Minkowski distance` is defined as:\n",
    "\n",
    "$D \\left( X, Y \\right) = \\left( \\sum_{i=1}^{n} {\\left| x_i - y_i \\right|}^p \\right)^{\\frac{1}{p}}$ for $p \\ge 1$\n",
    "\n",
    "<br>\n",
    "\n",
    "For $p = 1$:\n",
    "\n",
    "$D \\left( X, Y \\right) = \\sum_{i=1}^{n} {\\left| x_i - y_i \\right|} $ &nbsp;&nbsp; $ \\dots $ `Manhattan distance`\n",
    "\n",
    "<br>\n",
    "\n",
    "for $p = 2$:\n",
    "\n",
    "$D \\left( X, Y \\right) = \\sqrt {\\left( \\sum_{i=1}^{n} {\\left| x_i - y_i \\right|}^2 \\right) }$ &nbsp;&nbsp; $ \\dots $ `Euclidian distance`\n",
    "<br><br>\n",
    "\n",
    "<img src=\"./img/5_minkowski.png\" width=\"1000px\">\n",
    "\n",
    "<span style=\"font-size: 70%\">Unit circles (distance from the centre) for various <i>p</i>.<br>Source: <a href=\"https://en.wikipedia.org/wiki/Minkowski_distance\">Wikipedia</a></span>\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>To get a glimpse of the underlying implementation, you may want to read Chapter 12 in the book \"<b>Data Science from Scratch</b>\"</li>\n",
    "<li>For a primer with <b>scikit-learn</b> refer to <a href=\"https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms\">the Scikit Learn documentation on Nearest Neighbours Classifiers</a></li>\n",
    "<li>Scikit <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\">kNN Classifier</a></li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get acquainted with your data - The IRIS dataset\n",
    "\n",
    "This is the original [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "However, we will use the [scikit-learn iris toy-set](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) to see how to utilize the dataset loading facilities of scikit-learn.\n",
    "\n",
    "##### Let's take a look at what we have and what it means\n",
    "\n",
    "__Attribute Information:__\n",
    "\n",
    "1. sepal length \\[cm\\]\n",
    "2. sepal width \\[cm\\]\n",
    "3. petal length \\[cm\\]\n",
    "4. petal width \\[cm\\]\n",
    "5. class:\n",
    " - Iris Setosa (setosa)\n",
    " - Iris Versicolour (versicolor)\n",
    " - Iris Virginica (virginica)\n",
    " \n",
    "Total data records: 150 (50/species)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "__What they look like:__\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/78/Petal-sepal.jpg/226px-Petal-sepal.jpg\"><br>\n",
    "<span style=\"font-size: 70%\">Figure 1: Sepal (protective outer sheet) and Petal (blossom leaves)</span>\n",
    "<br><br>\n",
    "\n",
    "<table style=\"align: left; padding: 0; border-style: none\">\n",
    "    <tr style=\"border-style: none\">\n",
    "        <td style=\"border-style: none\">\n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/180px-Kosaciec_szczecinkowaty_Iris_setosa.jpg\">\n",
    "        </td>\n",
    "        <td style=\"border-style: none\">\n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/320px-Iris_versicolor_3.jpg\">\n",
    "        </td>\n",
    "        <td style=\"border-style: none\">\n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/295px-Iris_virginica.jpg\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr style=\"border-style: none\">\n",
    "        <td style=\"border-style: none\">Setosa</td>\n",
    "        <td style=\"border-style: none\">Versicolour</td>\n",
    "        <td style=\"border-style: none\">Virginica</td>\n",
    "    <tr>\n",
    "</table>\n",
    "<span style=\"font-size: 70%\">Figure 2: Different species of irises (classes) in the dataset</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "iris = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for statistic evaluation\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "df = pd.DataFrame(iris.data, columns=feature_columns)\n",
    "\n",
    "# we could do this\n",
    "# df['class'] = pd.Series(iris.target)\n",
    "# or use the LabelEncoder\n",
    "\n",
    "df['Species'] = pd.Series(iris.target_names[iris.target])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution\n",
    "df.boxplot(by=\"Species\", figsize=(10,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid')\n",
    "g = sns.pairplot(df, hue='Species', markers=[\"o\", \"s\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for model building with scikit-learn\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(f'Accuracy of our model is equal {str(round(accuracy, 2))} %.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the model decide?\n",
    "(drawing decision boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for display of decission boundaries\n",
    "from itertools import product\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# all combinations to compare\n",
    "X_cols_combo = [[\"SepalLengthCm\", \"SepalWidthCm\"], [\"SepalLengthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalLengthCm\", \"PetalWidthCm\"], [\"SepalWidthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalWidthCm\", \"PetalWidthCm\"], [\"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# comparing datasets\n",
    "X1 = df[X_cols_combo[0]].to_numpy()\n",
    "X2 = df[X_cols_combo[1]].to_numpy()\n",
    "X3 = df[X_cols_combo[2]].to_numpy()\n",
    "X4 = df[X_cols_combo[3]].to_numpy()\n",
    "X5 = df[X_cols_combo[4]].to_numpy()\n",
    "X6 = df[X_cols_combo[5]].to_numpy()\n",
    "y = iris.target\n",
    "\n",
    "# generating a forrest of classifiers\n",
    "clf_1 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_3 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_4 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_5 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf_6 = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# make things iterable\n",
    "clf_all = [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6]\n",
    "X_sets = [X1, X2, X3, X4, X5, X6]\n",
    "\n",
    "# helper to generate the image position\n",
    "img_pos = [pos_t for pos_t in product([0, 1, 2], [0, 1])]\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "for i, classifier in enumerate(clf_all):\n",
    "    classifier.fit(X_sets[i], y)\n",
    "    DecisionBoundaryDisplay.from_estimator(classifier, X_sets[i], alpha=0.4, ax=ax[img_pos[i][0], img_pos[i][1]], response_method=\"predict\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].scatter(X_sets[i][:, 0], X_sets[i][:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].set_title(f\"{X_cols_combo[i][0][:-2]} vs. {X_cols_combo[i][1][:-2]}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the implementation\n",
    "\n",
    "In the book only a very simple algorithm based on Euclidian distance was implemented.\n",
    "\n",
    "Using `Scikit-learn` has several benefits:\n",
    "\n",
    "- selection of optimal algorithms based on the characteristics of the dataset (ref. discussion of BruteForce, Ball Tree vs. KD Tree in the [online documentation](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms))\n",
    "- algorithmic handling of sparse and dense datasets (one does not have to correct NaN data)\n",
    "- adjustable calculation of metrics (specifically using RadiusNeighborsClassifier in case none-uniform distribution)\n",
    "- parameterized calculation cost (ref. discussion on leaf_size)\n",
    "\n",
    "Some things to keep in mind:\n",
    "\n",
    "- In rare cases, the order of data can determine the class of a data point\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "\n",
    "`k-Nearest Neighbour` as demonstrated in the book is susceptible to the `Curse of Dimensionality`.\n",
    "\n",
    "__Briefly explained:__\n",
    "\n",
    "With every dimension added to the data (and that implies, with every added feature), \n",
    "\n",
    "- computation time increases by an order $O \\left( N \\right)$\n",
    "- data density decreases\n",
    "- distance between data points grows\n",
    "- which leads to a spread between closest to average distances (making the range of data computationally complex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_students_input.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><h5>Students task: Discuss</h5>\n",
    "<ul>\n",
    "    <li>How would an algorithm have to be designed so that it is not vulnerable to this phenomenon?</li>\n",
    "    <li>Considering technological advances (in computing power), is this even an issue?</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Read Chapter 12, pages 223 - 228 in the book \"<b>Data Science from Scratch</b>\"</li>\n",
    "<li><a href=\"https://builtin.com/data-science/curse-dimensionality\">What Is the Curse of Dimensionality?</a> (also available in the material section)</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
