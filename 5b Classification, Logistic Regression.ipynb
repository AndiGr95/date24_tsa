{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "<img src=\"./img/5_linear-regression-vs-logistic-regression.png\"><br><br>\n",
    "<span style=\"font-size: 70%\">Source: [javaTpoint](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning)</span>\n",
    "<br><br>\n",
    "\n",
    "If our problem at hand is to distinguish between to target variables $y=0$ and $y=1$, using `Linear regression` cannot solve it.\n",
    "\n",
    "$y = \\theta_0 + \\theta_1 \\cdot x + \\epsilon $ &nbsp; will generate a continuous $y$ (ranging from $-inf$ to $inf$)\n",
    "\n",
    "What we need is a function that can discriminate between $ [0, 1] $.\n",
    "\n",
    "<img src=\"./img/5_linear-regression-vs-logistic-regression3.png\"><br><br>\n",
    "<span style=\"font-size: 70%\">Source: [javaTpoint](https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning)</span>\n",
    "<br><br>\n",
    "\n",
    "Such a function can be described as:\n",
    "\n",
    "$ \\log \\left [ \\frac{y}{1 - y} \\right ] = \\theta_0 + \\theta_1 \\cdot x $ \n",
    "\n",
    "or\n",
    "\n",
    "$ y = \\frac{1}{\\left ( 1 + e^{ (-x) } \\right)} $ &nbsp; ... sigmoid (logistic) function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take some samle data\n",
    "tuples = np.loadtxt(\"./data/paid_accounts.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "x_0 = tuples[:, 0][np.where(tuples[:, 2] == 0)]\n",
    "x_1 = tuples[:, 0][np.where(tuples[:, 2] == 1)]\n",
    "\n",
    "y_0 = tuples[:, 1][np.where(tuples[:, 2] == 0)]\n",
    "y_1 = tuples[:, 1][np.where(tuples[:, 2] == 1)]\n",
    "\n",
    "plt.scatter(x_0, y_0, c=\"#bb0000\")\n",
    "plt.scatter(x_1, y_1, c=\"#008800\")\n",
    "plt.title(\"\\nWhen is a person likely to pay for premium services?\\n\")\n",
    "plt.legend([\"unpaid\", \"paid\"])\n",
    "plt.xlabel(\"Experience [y]\")\n",
    "plt.ylabel(\"Anual salary [â‚¬]\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 70%\">It is good practice (but additional labour) to add axis labels and legends to the graph.</span>\n",
    "<br><br>\n",
    "\n",
    "__Interpretation__: Lower paid people are more likely to pay for premium services than people with higher income.\n",
    "\n",
    "Can we predict that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tuples[:, :2]\n",
    "y = tuples[:, 2]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "plt.scatter(y_pred, y)\n",
    "plt.title(\"\\nLinear Regression cannot predict discrete outcome!\\n\")\n",
    "plt.xlabel(\"predicted y (continuous)\")\n",
    "plt.ylabel(\"actual y (discrete)\")\n",
    "plt.show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 70%\">__Note__: The book \"__Data Science from Scratch__\" uses native Python to demonstrate this. We use `Scikit Learn` which provides the same results for didactic purposes.</span>\n",
    "\n",
    "Not helpful.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We look for a function that returns:\n",
    "\n",
    "$y \\in \\left [ 0, 1 \\right]$ \n",
    "\n",
    "$ \\to $ and thus our\n",
    "\n",
    "Hypothesis: \n",
    "\n",
    "$ 0 \\le h_\\theta \\left( x \\right) \\le 1 $\n",
    "\n",
    "$ h_\\theta \\left( x \\right) = g \\left( \\theta^T \\cdot x \\right) $ &nbsp; ... with $ g \\left( z \\right) = \\frac{1}{1 + e^{(-z)}} $\n",
    "\n",
    "Our Hypothesis should have probabilities:\n",
    "\n",
    "$ h_\\theta \\left( x \\right) = P \\left( y = 1 | x \\ge 0 \\right) $ &nbsp; and &nbsp; $ = 1 - P \\left( y = 0 | x \\lt 0 \\right) $\n",
    "\n",
    "$ \\to P \\left( y = 1 | x \\ge 0 \\right) + P \\left( y = 0 | x \\lt 0 \\right) = 1 $\n",
    "\n",
    "Decision boundaries:\n",
    "\n",
    "> &nbsp;  \n",
    "> $ h_\\theta \\left( x \\right) \\ge 0.5 $ &nbsp; $ \\to $ &nbsp; $ y = 1 $\n",
    "> <br>  \n",
    "> $ h_\\theta \\left( x \\right) \\lt 0.5 $ &nbsp; $ \\to $ &nbsp; $ y = 0 $\n",
    "> <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-10, 10, 0.01)\n",
    "y = 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "plt.grid(visible=True)\n",
    "plt.plot(X, y)\n",
    "plt.title(r\"Sigmoid (or Logistic) function $y = \\frac {1}{1+e^{-x}}$\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function:\n",
    "\n",
    "$ J \\left( \\theta \\right) = \\frac{1}{m} \\cdot \\sum \\left[ y \\cdot \\log \\left( h_\\theta \\left( x \\right) \\right) + (1 - y) \\cdot \\left( 1 - h_\\theta \\left( x \\right) \\right) \\right] $\n",
    "<br>\n",
    "\n",
    "##### Vectorized:\n",
    "\n",
    "> &nbsp;  \n",
    "> Hypothesis: &nbsp; $ h = g \\left( X \\cdot \\theta \\right) $\n",
    "> <br><br>\n",
    "\n",
    "> &nbsp;  \n",
    "> Cost function: &nbsp; $ J \\left( \\theta \\right) = \\frac{1}{m} \\cdot \\left( - y^T \\cdot \\log \\left( h \\right) - (1 - y)^T \\cdot \\left( 1 - h \\right) \\right) $\n",
    "> <br><br>\n",
    "\n",
    "<br>\n",
    "Gradient descent:\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta_0} \\cdot J \\left( \\theta \\right) $\n",
    "\n",
    "$ \\theta_j = \\theta_j - \\frac{\\alpha}{m} \\cdot \\sum_{i=1}^{m} \\left( h_\\theta \\left( x^{(i)} \\right) - y \\right) \\cdot x_j $\n",
    "\n",
    "<br>\n",
    "\n",
    "Recall that parameter updates should be conducted ___all at once___.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### $ \\to $ Vectorized gradient descent\n",
    "\n",
    "> &nbsp;  \n",
    "> Parameters: &nbsp; $ \\theta = \\theta - \\frac{\\alpha}{m} \\cdot X^T \\cdot \\left( g \\left( X \\cdot \\theta - \\overrightarrow{y} \\right) \\right) $\n",
    "> <br><br>\n",
    "\n",
    "> &nbsp;  \n",
    "> Cost function: &nbsp; $ \\nabla J\\left( \\theta \\right) = \\frac{1}{m} \\cdot X^T \\cdot \\left( X \\cdot \\theta - \\overrightarrow y \\right) $\n",
    "> <br><br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./img/0_critical_evaluation.png\" width=\"150px\">\n",
    "\n",
    "### A final word of warning\n",
    "\n",
    "`Logistic regression` can __distinguish only between two__ classes.\n",
    "\n",
    "To circumvent this, a model is trained in a __One versus Rest__ fashion.\n",
    "\n",
    "Separate models are ___trained for each class___  \n",
    "predicting whether an observation is true for that specific class  \n",
    "effectively making it a ___binary classification problem___.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS Classification using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for model building with scikit-learn\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation\n",
    "# for details on solvers, refer to https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "# for documentation on LogisticRegression() refer to https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, log.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print(f'Accuracy of our model is equal {str(round(accuracy, 2))} %.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for display of decission boundaries\n",
    "from itertools import product\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# create a dataframe for statistic evaluation\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "df = pd.DataFrame(iris.data, columns=feature_columns)\n",
    "df['Species'] = pd.Series(iris.target_names[iris.target])\n",
    "\n",
    "# all combinations to compare\n",
    "X_cols_combo = [[\"SepalLengthCm\", \"SepalWidthCm\"], [\"SepalLengthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalLengthCm\", \"PetalWidthCm\"], [\"SepalWidthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalWidthCm\", \"PetalWidthCm\"], [\"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# comparing datasets\n",
    "X1 = df[X_cols_combo[0]].to_numpy()\n",
    "X2 = df[X_cols_combo[1]].to_numpy()\n",
    "X3 = df[X_cols_combo[2]].to_numpy()\n",
    "X4 = df[X_cols_combo[3]].to_numpy()\n",
    "X5 = df[X_cols_combo[4]].to_numpy()\n",
    "X6 = df[X_cols_combo[5]].to_numpy()\n",
    "y = iris.target\n",
    "\n",
    "# generating a forrest of classifiers\n",
    "clf_1 = LogisticRegression(max_iter=1000)\n",
    "clf_2 = LogisticRegression(max_iter=1000)\n",
    "clf_3 = LogisticRegression(max_iter=1000)\n",
    "clf_4 = LogisticRegression(max_iter=1000)\n",
    "clf_5 = LogisticRegression(max_iter=1000)\n",
    "clf_6 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# make things iterable\n",
    "clf_all = [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6]\n",
    "X_sets = [X1, X2, X3, X4, X5, X6]\n",
    "\n",
    "# helper to generate the image position\n",
    "img_pos = [pos_t for pos_t in product([0, 1, 2], [0, 1])]\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "for i, classifier in enumerate(clf_all):\n",
    "    classifier.fit(X_sets[i], y)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, X_sets[i], alpha=0.4, ax=ax[img_pos[i][0], img_pos[i][1]], response_method=\"predict\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].scatter(\n",
    "        X_sets[i][:, 0], X_sets[i][:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].set_title(\n",
    "        f\"{X_cols_combo[i][0][:-2]} vs. {X_cols_combo[i][1][:-2]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the implementation\n",
    "\n",
    "Using default parameters leads to convergence warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, one can change the ___number of iterations___, the model uses (similar to the epochs in Linear Regression with gradient descent).\n",
    "\n",
    "> &nbsp;  \n",
    "> logreg = LogisticRegression(max_iter=1000)\n",
    "> <br><br>\n",
    "\n",
    "Another option (with difficile impact on the classification results) is to use different - problem specific - ___solvers___. Several solvers can be used:\n",
    "\n",
    "> &nbsp;\n",
    "> - lbfgs\n",
    "> - liblinear\n",
    "> - newton-cg\n",
    "> - newton-cholesky\n",
    "> - sag\n",
    "> - saga\n",
    "> <br><br>\n",
    "\n",
    "Finally, Logistic Regression allows to penalize the outcome of the cost function (___penalty___) or adjust the tolerance of the stop criteria (___tol___).\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_students_input.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><h5>Students task: Experiment</h5>\n",
    "<ul>\n",
    "    <li>Adjust the maximum iterations. When does the warning disappear?</li>\n",
    "    <li>Experiment with the solver. Which solver provides the best results (highest accuracy)?</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Read about <a href=\"https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\">Logistic Regression</a> in Scikit Learn</li>\n",
    "<li>Read about <a href=\"https://scikit-learn.org/stable/modules/linear_model.html#solvers\">Solvers</a></li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "\n",
    "`Logistic Regression` is a little picky about the spread of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution\n",
    "df.boxplot(by=\"Species\", figsize=(10,7))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Petal length varies significantly between `Iris setosa` and `Iris virginica`.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"./img/5_feature_scaling.png\" width=\"700px\"><br><br>\n",
    "\n",
    "We can calculate:\n",
    "\n",
    "$ x_{js} = \\frac{x_j - \\mu_j}{s_j} $ &nbsp; ... with &nbsp; $ s_j = \\max \\left( x_j \\right) - \\min \\left( x_j \\right) $\n",
    "\n",
    "This leads to:\n",
    "\n",
    "<img src=\"./img/5_feature_scaling2.png\" width=\"550px\"><br><br>\n",
    "\n",
    "`Scikit Learn` offers a StandardScaler to provide feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "X = np.arange(6)\n",
    "y = np.array([10, 100000, 1000, 100, 200, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate normalized y_norm\n",
    "y_mean = np.mean(y)\n",
    "y_s = np.max(y) - np.min(y)\n",
    "y_norm = (y - y_mean)/y_s\n",
    "plt.barh(X, y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(y.reshape(-1, 1), X)\n",
    "\n",
    "y_scaled = scaler.transform(y.reshape(-1, 1))\n",
    "# if you want scaling within spread, uncomment the following line\n",
    "# y_scaled = y_scaled/(y_scaled.max() - y_scaled.min())\n",
    "\n",
    "plt.barh(X, y_scaled.reshape(1, -1)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler` scales according to:\n",
    "\n",
    "$ y_{scaled} = \\frac{\\left( x - \\mu \\right)}{\\sigma} $\n",
    "\n",
    "using standard deviation as the spread.\n",
    "\n",
    "Our normalized Python scaler used:\n",
    "\n",
    "$ y_{norm} = \\frac{\\left( x - \\mu \\right)}{ \\left( x_{max} - x_{min} \\right)} $\n",
    "\n",
    "If we want to achieve equal results with the `StandardScaler` we must normalize the scaled results:\n",
    "\n",
    "$ y_{scaled} = \\frac{\\left( x - \\mu \\right)}{\\sigma \\cdot \\left( x_{max} - x_{min} \\right)} $\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Scaling data is good practice.\n",
    "\n",
    "- It norms the feature spread of the data\n",
    "- Some algorithms have problems to converge using data of different scales $ \\to $ scaling helps these algorithms to converge\n",
    "- Scaled data might help reduce computational cost\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_students_input.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><h5>Students task:</h5>\n",
    "<ul>\n",
    "    <li>Verify that the results are indeed the same when normalizing the scaled values to the spread.</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/preprocessing.html\">Preprocessing data</a> on Scalers</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalizing our initial experiment\n",
    "\n",
    "How does this apply to our paid account example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# get data\n",
    "tuples = np.loadtxt(\"./data/paid_accounts.csv\", delimiter=\",\")\n",
    "\n",
    "X = tuples[:, :2]\n",
    "y = tuples[:, 2]\n",
    "\n",
    "# prepare - scale - data\n",
    "std_scaler = StandardScaler()\n",
    "X_scaled = std_scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\n",
    "\n",
    "logR = LogisticRegression()\n",
    "logR.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logR.predict(X_test)\n",
    "\n",
    "# model quality\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f'Confusion:\\n{cm}\\n')\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, logR.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')\n",
    "\n",
    "# decision boundary\n",
    "DecisionBoundaryDisplay.from_estimator(logR, X_scaled, alpha=0.4, response_method=\"predict\")\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y, s=30, edgecolor=\"k\")\n",
    "plt.title(\"\\nWhen is a person likely to pay for premium services?\\n\")\n",
    "plt.xlabel(\"Experience scaled [y]\")\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylabel(\"Anual salary scaled [â‚¬]\")\n",
    "plt.ylim (-3, 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further predictions\n",
    "\n",
    "The model is trained on scaled data. \n",
    "\n",
    "For further, real-world estimations, one has to scale the input prior to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume 1y experience and a salary of â‚¬ 50.000,-\n",
    "# the sample is similar to X[18, :]\n",
    "\n",
    "experience = 1\n",
    "salary = 56000\n",
    "\n",
    "X_verify = np.array([experience, salary]).reshape(1, -1)\n",
    "X_verify_scaled = std_scaler.transform(X_verify)\n",
    "\n",
    "y_verify_pred = logR.predict(X_verify_scaled)\n",
    "print(f\"Real data target: {y[18]:.0f}\\nPredicted target: {y_verify_pred[0]:.0f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 128px\">&#9749;</span> Coffee break!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
