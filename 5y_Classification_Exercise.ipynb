{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises in Classification (Solutions)\n",
    "\n",
    "In the code you will see:\n",
    "\n",
    "> &nbsp;  \n",
    "> \\# YOUR CODE HERE  \n",
    "> ...\n",
    "> <br><br>\n",
    "\n",
    "Insert your code to complete the exercise.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Performance Assessment for a Decision Tree based Model\n",
    "\n",
    "Examine the quality of the decision tree example from last lecture. Compare the quality measures that are calculated on the basis of the test data with those created by cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "# Load input data\n",
    "input_file = './data/data_decision_trees.txt'\n",
    "data = np.loadtxt(input_file, delimiter=',')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# correct the assigment\n",
    "X, y = data[:], data[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate input data into two classes based on labels\n",
    "class_0 = np.array(X[y==0])\n",
    "class_1 = np.array(X[y==1])\n",
    "\n",
    "# Visualize input data\n",
    "plt.figure()\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], s=75, facecolors='black', linewidth=1, marker='x')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], s=75, facecolors='white', \n",
    "        edgecolors='black', linewidth=1, marker='o')\n",
    "plt.title('Input data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing datasets (75% vs. 25%)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# complete the code\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(random_state=23)\n",
    "\n",
    "# Decision Trees classifier \n",
    "params = {'random_state': 0, 'max_depth': 10}\n",
    "dt_classifier = DecisionTreeClassifier(**params)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# complete the code\n",
    "dt_classifier.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw decision boundaries\n",
    "DecisionBoundaryDisplay.from_estimator(dt_classifier, X, alpha=0.4, response_method=\"predict\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolor=\"k\")\n",
    "plt.title(\"Decision boundary for Decision Tree Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy on the basis of the test data\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# generate a confusion matrix\n",
    "# YOUR CODE HERE\n",
    "cm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, dt_classifier.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy through cross-validation\n",
    "from sklearn import model_selection\n",
    "\n",
    "# this defines the cross-validation strategy (cv parameter)\n",
    "num_folds = 7\n",
    "\n",
    "# Scoring functions of sklearn (original data: X and y!)\n",
    "accuracy_values = model_selection.cross_val_score(dt_classifier, X, y, scoring='accuracy', cv=num_folds)\n",
    "print(f\"Accuracy:  {accuracy_values.mean():.2f}% -> {accuracy_values}\")\n",
    "\n",
    "precision_values = model_selection.cross_val_score(dt_classifier, X, y, scoring='precision_weighted', cv=num_folds)\n",
    "print(f\"Precision: {precision_values.mean():.2f}% -> {precision_values}\")\n",
    "\n",
    "recall_values = model_selection.cross_val_score(dt_classifier, X, y, scoring='recall_weighted', cv=num_folds)\n",
    "print(f\"Recall:    {recall_values.mean():.2f}% -> {recall_values}\")\n",
    "\n",
    "f1_values = model_selection.cross_val_score(dt_classifier, X, y, scoring='f1_weighted', cv=num_folds)\n",
    "print(f\"F1:        {f1_values.mean():.2f}  -> {f1_values}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Parameter Tuning for a kNN based Model\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "Vary the parameter k for a kNN based prediction model in order to maximize the accuracy. Calculate the measures using cross validation with 5 sets (folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data\n",
    "input_file = \"./data/data_decision_trees.txt\"\n",
    "data = np.loadtxt(input_file, delimiter=',')\n",
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input data into two classes based on labels\n",
    "class_0 = np.array(X[y==0])\n",
    "class_1 = np.array(X[y==1])\n",
    "\n",
    "# Visualize input data\n",
    "plt.figure()\n",
    "plt.scatter(class_0[:, 0], class_0[:, 1], s=75, facecolors='red', linewidth=1, marker='x')\n",
    "plt.scatter(class_1[:, 0], class_1[:, 1], s=75, facecolors='lightblue', edgecolors='black', linewidth=1, marker='o')\n",
    "plt.title('Input data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Split up the data into training and test data (ration: 3:1).\n",
    "# YOUR CODE HERE\n",
    "# correct the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=?, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of data-set:\", X.shape, \", training data:\",X_train.shape, \", test data:\", X_test.shape)\n",
    "print(\"Shape of label-data:\", y.shape, \", training labels:\",y_train.shape, \", test labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a kNN based classifier with k=1 and weights being distance and use the visualize_classifier\n",
    "# function to analyze the quality of the classifier.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "knn_clf = neighbors.KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a confusion matrix\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "cm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, knn_clf.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy through cross-validation\n",
    "from sklearn import model_selection\n",
    "\n",
    "# this defines the cross-validation strategy (cv parameter)\n",
    "num_folds = 3\n",
    "\n",
    "# Scoring functions of sklearn (original data: X and y!)\n",
    "accuracy_values = model_selection.cross_val_score(knn_clf, X, y, scoring='accuracy', cv=num_folds)\n",
    "print(f\"Accuracy:  {accuracy_values.mean():.2f}% -> {accuracy_values}\")\n",
    "\n",
    "precision_values = model_selection.cross_val_score(knn_clf, X, y, scoring='precision_weighted', cv=num_folds)\n",
    "print(f\"Precision: {precision_values.mean():.2f}% -> {precision_values}\")\n",
    "\n",
    "recall_values = model_selection.cross_val_score(knn_clf, X, y, scoring='recall_weighted', cv=num_folds)\n",
    "print(f\"Recall:    {recall_values.mean():.2f}% -> {recall_values}\")\n",
    "\n",
    "f1_values = model_selection.cross_val_score(knn_clf, X, y, scoring='f1_weighted', cv=num_folds)\n",
    "print(f\"F1:        {f1_values.mean():.2f}  -> {f1_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# run k from 1 to 15\n",
    "for k in np.arange(1, 16):\n",
    "    knn_clf = neighbors.KNeighborsClassifier(k, weights=\"distance\")\n",
    "    knn_clf.fit(X_train, y_train)\n",
    "    print(knn_clf)\n",
    "    \n",
    "    y_pred = knn_clf.predict(X_test)\n",
    "    accuracy_values = model_selection.cross_val_score(knn_clf, X, y, scoring='accuracy', cv=num_folds)\n",
    "\n",
    "    print('Accuracy:')\n",
    "    print(f' Train: {accuracy_score(y_train, knn_clf.predict(X_train))*100:.2f}%')\n",
    "    print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "    print(f\" CV:    {accuracy_values.mean()*100:.2f}%\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Multiclass ensemble classification\n",
    "\n",
    "##### Like some Pizza?\n",
    "\n",
    "<img src=\"./img/5_pizza.webp\" width=\"500px\"><br><br>\n",
    "\n",
    "Can we distinguish the brand from the ingredients?\n",
    "\n",
    "### The Pizza dataset\n",
    "\n",
    "__Attributes__:\n",
    "\n",
    "1. brand ... Pizza Brand [A ... J] (no ads allowed)\n",
    "1. id .......... Sample ID\n",
    "1. mois ..... Moisture, amount of water per 100g in the sample\n",
    "1. prot ...... Proteins per 100g\n",
    "1. fat ........ Fat per 100g\n",
    "1. ash ....... Ash (yes) per 100g\n",
    "1. sodium . Sodium per 100g\n",
    "1. carb ..... Carbohydrates per 100\n",
    "1. cal ........ Calories per 100\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/luishpinto/multiclass-classification/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from itertools import product\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# model preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains mixed text and numerical values.\n",
    "\n",
    "In `Numpy`, the easiest way to load the data is to select features and targets directly ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data using numpy\n",
    "X = np.loadtxt(\"./data/Pizza.csv\", skiprows=1, usecols=(2, 3, 4, 5, 6, 7, 8), delimiter=\",\")\n",
    "brand = np.loadtxt(\"./data/Pizza.csv\", skiprows=1, dtype=\"str\", usecols=(0), delimiter=\",\")\n",
    "\n",
    "# convert string labels to numerical targets\n",
    "brand_encoder = LabelEncoder()\n",
    "y = brand_encoder.fit_transform(brand)\n",
    "brand_names = sorted(list(set(brand)))\n",
    "\n",
    "X, y, brand_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... while in `Pandas` you can read the data directly and select the required columns later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "data = pd.read_csv(\"./data/Pizza.csv\")\n",
    "\n",
    "X = data.iloc[:, 2:9].to_numpy()\n",
    "# in pandas converting alphanumerical categories into numerical, use factorize\n",
    "y = pd.factorize(data.iloc[:, 0])[0]\n",
    "\n",
    "X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, you have `X` and `y` as numpy arrays.\n",
    "\n",
    "Let's do some basic statistics to get acquainted to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataframe for analysis\n",
    "df = pd.concat([data, pd.Series(y, name=\"brand_id\")], axis=1)\n",
    "df.drop(labels=[\"brand\", \"id\"], axis=1, inplace=True)\n",
    "\n",
    "# generate pair plot\n",
    "sns.set_theme(style='darkgrid')\n",
    "g = sns.pairplot(df, hue='brand_id', palette=sns.color_palette())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many dimensions! We will learn to reduce them in `Clustering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"kNN\", \"Logistic Regression\", \"SVM\", \"Decission Tree\", \"RandomForest\"]\n",
    "# YOUR CODE HERE\n",
    "# # define classifiers with reasonable parameters:\n",
    "#   KNeighborsClassifier -->           k=3\n",
    "#   LogisticRegression -->             max_iter=2000\n",
    "#   Decision Trees and RandomFores --> max_depth=5\n",
    "\n",
    "classifiers = [KNeighborsClassifier(), LogisticRegression(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make copy for later use\n",
    "bounds_clf = copy.deepcopy(classifiers)\n",
    "scores = []\n",
    "cms = []\n",
    "crs = []\n",
    "\n",
    "models_trained = False\n",
    "\n",
    "# prepare the train / test split, 80% training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "try:\n",
    "    for name, classifier in zip(names, classifiers):\n",
    "\n",
    "        # train the classifier\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        clf = None\n",
    "        \n",
    "        # predict results\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        y_pred = None\n",
    "\n",
    "        # compute the confusion matrix\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        cm = None\n",
    "        \n",
    "        # compute the classification report\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        cr = classification_report(?, ?, target_names=brand_names, zero_division=0)\n",
    "\n",
    "        # collect data about classifiers\n",
    "        scores.append(clf.score(X_test, y_test))\n",
    "        cms.append(cm)\n",
    "        crs.append(cr)\n",
    "    \n",
    "    models_trained = True\n",
    "except:\n",
    "    print(\"You should complete the code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_trained:\n",
    "    f = plt.figure(figsize=(21, 3))\n",
    "    for i in range(len(names)):\n",
    "        ax = plt.subplot(1, 5, i+1)\n",
    "        sns.heatmap(cm, annot=True, cmap=\"Greens\", cbar=False, ax=ax, xticklabels=brand_names, yticklabels=brand_names)\n",
    "        ax.set_title(f\"{names[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_trained:\n",
    "    fi = classifiers[4].feature_importances_\n",
    "    print(f\"Feature importance:\\n{fi}\")\n",
    "\n",
    "    top_features = sorted(fi, reverse=True)[:2]\n",
    "    feature_1 = [i for i, x in enumerate(top_features) if x == top_features[0]][0]\n",
    "    feature_2 = [i for i, x in enumerate(top_features) if x == top_features[1]][0]\n",
    "\n",
    "    print(f\"Features used in calculating decision bounds: {feature_1}, {feature_2}\")\n",
    "\n",
    "    X_select = df.iloc[:, [feature_1, feature_2]].to_numpy()\n",
    "    X_1 = X_select[:, 0]\n",
    "    X_2 = X_select[:, 1]\n",
    "\n",
    "    # bounds_clf = classifiers.copy()\n",
    "    bounds_clf[1].set_params(**{\"max_iter\": 10000})\n",
    "\n",
    "    f = plt.figure(figsize=(21, 3))\n",
    "    for i in range(len(names)):\n",
    "        clf = bounds_clf[i].fit(X_select, y)\n",
    "        ax = plt.subplot(1, 5, i+1)\n",
    "        DecisionBoundaryDisplay.from_estimator(clf, X_select, ax=ax, alpha=0.3, response_method=\"predict\")\n",
    "        ax.scatter(X_1, X_2, s=20, edgecolors=\"darkblue\")\n",
    "        ax.set_title(names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_trained:\n",
    "    best_model = [i for i, x in enumerate(scores) if x == max(scores)][0]\n",
    "    print(f\"Accuracy scores of all models:\\n  {scores}\\n\")\n",
    "    print(f\"Best results:\\n  {names[best_model]} with {max(scores):.4f}%\\n\")\n",
    "    print(f\"Classification Report of {names[best_model]}:\\n\\n{crs[best_model]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we want a juicy pizza that is rich of proteins.\n",
    "\n",
    "Which brand would our model suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prediction parameters\n",
    "mois = 48.0                     # try 30.0 for dry pizza\n",
    "prot = 22.5                     # try 8.0 for lean pizza\n",
    "\n",
    "if models_trained:\n",
    "    y_pred = bounds_clf[best_model].predict([[mois, prot]])\n",
    "\n",
    "    print(f\"Our model would suggest brand {brand_names[y_pred[0]]}\")\n",
    "else:\n",
    "    print(\"Our model would suggest something if it were trained properly.\\nPlease train the model first\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gratulations!\n",
    "\n",
    "If your notebook runs without errors till here, you have successfully commenced the exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
