{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "<img src=\"./img/5_decision_tree.png\" width=\"700px\"><br><br>\n",
    "\n",
    "### Definition:\n",
    "\n",
    "_\"A decision tree is a non-parametric supervised learning algorithm, ... .  \n",
    "It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\"_\n",
    "\n",
    "<span style=\"font-size: 70%\">Source: <a href=\"https://www.ibm.com/topics/decision-trees\">IBM</a></span><br><br>\n",
    "\n",
    "There are several algorithms that implement `Decision Trees`:\n",
    "\n",
    "> &nbsp;\n",
    "> - __ID3__ (`Iterative Dichotomiser 3`): This algorithm leverages __entropy__ and __information gain__ as metrics to evaluate candidate splits.<br><br>\n",
    "> - __C4.5__: Advanced ID3 implementation that uses __information gain__ or __gain ratios__ to evaluate split points.<br><br>\n",
    "> - __C5.0__: Latest ID3 implementation released under a proprietary license<br><br>\n",
    "> - __CART__ (`Classification and Regression Trees`): This algorithm typically utilizes __Gini impurity__ to identify the ideal attribute to split on.<br>Gini impurity measures how often a randomly chosen attribute is misclassified (smaller is better).\n",
    "> <br><br>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Some theory\n",
    "\n",
    "__Def: Entropy__\n",
    "\n",
    "$ Entropy \\left( S \\right) = - \\sum_{c \\in C} p (c) \\log_2 p (c) $ \n",
    "\n",
    "with \n",
    "\n",
    "$ S $ ... data set  \n",
    "$ c $ ... classes in the set $ S $  \n",
    "$ p(c) $ ... proportion of data points of class $ c $ to total data points in $ S $\n",
    "\n",
    "$ Entropy \\left( s \\right) ... \\> [0, 1] $\n",
    "\n",
    "<br><br>\n",
    "\n",
    "__Def: Information Gain__\n",
    "\n",
    "$ Information Gain \\left( S, \\alpha \\right) = Entropy \\left( S \\right) -\\sum_{v \\in C} \\dfrac{ \\left| S_v \\right| }{ S } Entropy \\left( S_v \\right) $\n",
    "\n",
    "with\n",
    "\n",
    "$ \\alpha $ ... specific attribute or class  \n",
    "$ \\dfrac{ \\left| S_v \\right| }{ S } $ ... portion of values in $ S_v $ to the total number of values in $ S $  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "__DEF: Gini Impurity index__\n",
    "\n",
    "$ Gini = 1 - \\sum_i \\left( p_i \\right)^2 $\n",
    "\n",
    "with\n",
    "\n",
    "$ p_i $ ... probability of class $ i $\n",
    "\n",
    "<span style=\"font-size: 70%\">Source: <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\">Wikipedia</a></span>\n",
    "\n",
    "##### Comparison of selection characteristics\n",
    "\n",
    "<img src=\"./img/5_entropy_vs_gini.png\" width=\"700px\"><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Read about <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\">Decision Tree learning</a> in Wikipedia,</li>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/tree.html#decision-trees\">Decision Trees</a> in Scikit learn</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__\n",
    "\n",
    "`Scikit learn` offers the output of decision trees in graphical form. To benefit from this, you need to install `graphviz` on you computer and `pydot` in your Python environment.\n",
    "\n",
    "1. Go to [Graphviz](https://graphviz.org/download) to install `graphviz` library.\n",
    "\n",
    "1. <pre>pip install -U pyplot</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install pyplot, uncomment the following line\n",
    "\n",
    "# !pip install -U pyplot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS Classification using Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obligatory imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the data already, we need not repeat data analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model quality\n",
    "\n",
    "y_pred=dt.predict(X_test)\n",
    "\n",
    "# generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, dt.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for display of decission boundaries\n",
    "from itertools import product\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# create a dataframe for statistic evaluation\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "df = pd.DataFrame(iris.data, columns=feature_columns)\n",
    "df['Species'] = pd.Series(iris.target_names[iris.target])\n",
    "\n",
    "# all combinations to compare\n",
    "X_cols_combo = [[\"SepalLengthCm\", \"SepalWidthCm\"], [\"SepalLengthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalLengthCm\", \"PetalWidthCm\"], [\"SepalWidthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalWidthCm\", \"PetalWidthCm\"], [\"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# comparing datasets\n",
    "X1 = df[X_cols_combo[0]].to_numpy()\n",
    "X2 = df[X_cols_combo[1]].to_numpy()\n",
    "X3 = df[X_cols_combo[2]].to_numpy()\n",
    "X4 = df[X_cols_combo[3]].to_numpy()\n",
    "X5 = df[X_cols_combo[4]].to_numpy()\n",
    "X6 = df[X_cols_combo[5]].to_numpy()\n",
    "y = iris.target\n",
    "\n",
    "# generating a forrest of classifiers\n",
    "clf_1 = DecisionTreeClassifier()\n",
    "clf_2 = DecisionTreeClassifier()\n",
    "clf_3 = DecisionTreeClassifier()\n",
    "clf_4 = DecisionTreeClassifier()\n",
    "clf_5 = DecisionTreeClassifier()\n",
    "clf_6 = DecisionTreeClassifier()\n",
    "\n",
    "# make things iterable\n",
    "clf_all = [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6]\n",
    "X_sets = [X1, X2, X3, X4, X5, X6]\n",
    "\n",
    "# helper to generate the image position\n",
    "img_pos = [pos_t for pos_t in product([0, 1, 2], [0, 1])]\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "for i, classifier in enumerate(clf_all):\n",
    "    classifier.fit(X_sets[i], y)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, X_sets[i], alpha=0.4, ax=ax[img_pos[i][0], img_pos[i][1]], response_method=\"predict\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].scatter(X_sets[i][:, 0], X_sets[i][:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].set_title(f\"{X_cols_combo[i][0][:-2]} vs. {X_cols_combo[i][1][:-2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision tree\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_tree(decision_tree=dt)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "Ensemble methods combine several base estimators to provide better generalization and robustness over single purpose estimators.\n",
    "\n",
    "One such ensemble method (not exclusively) is ...\n",
    "\n",
    "### Random Forest Trees\n",
    "\n",
    "<img src=\"./img/5_Random_forest_diagram_complete.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Read about <a href=\"https://scikit-learn.org/stable/modules/ensemble.html#forest\">Ensemble methods</a> in Scikit learn</li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Random_forest\">Random forest</a> in Wikipedia,</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if `Random forest` classifiers improve our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model quality\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, rf.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RandomForestClassifier uses {len(rf.estimators_)} Decision Tree Classifiers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest provides feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, features 3 (Petal length) and 4 (Petal width) contribute the most to the decision process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pos = [pos for pos in product([0, 1], [0, 1, 2, 3, 4])]\n",
    "estimators = [0, 1, 2, 3, 4, 10, 50, 90, 98, 99]\n",
    "\n",
    "f, ax = plt.subplots(2, 5, figsize=(20, 10))\n",
    "\n",
    "f.suptitle(\"Selection of Decision trees in the Random forest Classifier\")\n",
    "for i, estimator in enumerate(estimators):\n",
    "    plot_tree(rf.estimators_[estimator], ax=ax[img_pos[i][0]][img_pos[i][1]])\n",
    "    ax[img_pos[i][0]][img_pos[i][1]].set_title(f\"Decision Tree #{estimator}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/0_critical_evaluation.png\" width=\"150px\">\n",
    "\n",
    "### Critical evaluation\n",
    "\n",
    "In this example, `Decision Trees` and `Random Forests` provide similar accuracy.\n",
    "\n",
    "In general, ensemble methods provide better generalization over single method models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
