{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "<img src=\"./img/6_classification_vs_clustering.png\" width=\"700px\"><br><br>\n",
    "\n",
    "With `Classification` we want to find a hyperplane that __best separates__ data into similar categories.  \n",
    "Classification requires __labeled__ data.\n",
    "\n",
    "With `Clustering` we want to group __unlabeled__ data into __similar__ sets of examples.  \n",
    "Before you can classify or group similar examples, you need to define __similarity__.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Assume a set of unlabeled data. You may want to find similarities (within the features).\n",
    "\n",
    "<img src=\"./img/6_Clustering-example-with-intra-and-inter-clustering-illustrations.png\" width=\"500px\"><br>\n",
    "\n",
    "<span style=\"font-size: 70%\">Source: <a href=\"https://www.researchgate.net/figure/Clustering-example-with-intra-and-inter-clustering-illustrations_fig1_344590665\">ResearchGate</a></span><br>\n",
    "\n",
    "Examples:\n",
    "\n",
    "- market segmentation (buyers segmented by income)\n",
    "- social network analysis (sentiment analysis of contributions)\n",
    "- recommendation groupings (commerce applications)\n",
    "- astronomy (dark matter detection, categorisation of stars by brightness and red-shift)\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sample data-set\n",
    "X = np.loadtxt('./data/data_clustering.txt', delimiter=',')\n",
    "\n",
    "# Plot input data\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to identify the intrinsic properties of data points that make them belong to the same group - `clusters`. \n",
    "\n",
    "In order to find these `clusters`, we use some kind of __similarity measures__ such as __Euclidean distance__. These similarity measure can estimate the tightness of a cluster.  \n",
    "\n",
    "There is no universal similarity metric that works for all the cases.\n",
    "\n",
    "It depends on the problem at hand. For example, we might be interested in finding the representative data point for each group or we might be interested in finding the outliers in our data. Depending on the situation, we will end up choosing the appropriate metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Cluster\n",
    "\n",
    "A simple but effective clustering algorithm is `k-Means`.\n",
    "\n",
    "`k-means` follows these steps:\n",
    "\n",
    "> &nbsp;\n",
    "> 1. define $ k  \\gt 1 $ to group data into $ k $ clusters ($ k $ ... hyperparameter)<br>  \n",
    "> 2. define $ k $ _cluster centroids_ $ c_i $ (either manually or algorithmically) ... $ max \\left( distance \\left( k_i - k_j \\right) \\right) $  \n",
    "_repeat until equilibrium is reached:_  \n",
    "> 3. &nbsp;&nbsp;&nbsp;&nbsp;calculate minimum distance of data points to clusters and assign each point a _centroid_ based on the distance $ c_x = min_{i=1..k} \\left( x_j - c_i \\right) $\n",
    "> 4. &nbsp;&nbsp;&nbsp;&nbsp;calculate the mean of all data points in a cluster and assign this mean as the new centroid  \n",
    "> <br><br>\n",
    "\n",
    "These $ k $ _centroids_ are used for inference (prediction).\n",
    "<br><br>\n",
    "\n",
    "<img src=\"./img/6_K-means_convergence.gif\"><br><br>\n",
    "<span style=\"font-size: 70%\">k-Means convergence over the iterations. If the centroids don't change beyond a threshold (<b>iterations 12-14</b>), the algorithm has converged and stops.<br>\n",
    "Source: <a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">Wikipedia</a></span>\n",
    "<br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try `k-Means`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the obligatory imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X = np.loadtxt('./data/data_clustering.txt', delimiter=',')\n",
    "\n",
    "# prepare data\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define k as hyperparameter, take a wild guess\n",
    "k = 5\n",
    "\n",
    "# model generation\n",
    "kM = KMeans(n_clusters=k, n_init=\"auto\")\n",
    "kM.fit(X_train)\n",
    "train_labels = kM.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "test_labels = kM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing data and decision boundaries\n",
    "DecisionBoundaryDisplay.from_estimator(kM, X, alpha=0.4, response_method=\"predict\")\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=train_labels)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=test_labels, alpha=0.5, edgecolors=\"#666666\")\n",
    "plt.scatter(kM.cluster_centers_[:, 0], kM.cluster_centers_[:, 1], c=\"#ff0000\", marker=\"x\", s=140)\n",
    "plt.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best $ k $\n",
    "\n",
    "What is the best $ k $ ?\n",
    "\n",
    "If the data is naturally organized into a number of distinct clusters, then it is easy to visually examine it and draw some inferences. But this is rarely the case in the real world. The data in the real world is huge and messy. So we need a way to quantify the quality of the clustering.\n",
    "\n",
    "##### Silhouette score\n",
    "\n",
    "`Silhouette` refers to a method used to check the consistency of clusters in our data. It gives an estimate of how well each data point fits with its cluster.\n",
    "\n",
    "The `silhouette score` is a __metric__ that measures how similar a data point is to its own cluster, as compared to other clusters. The silhouette score works with any similarity metric.\n",
    "<br><br>\n",
    "\n",
    "For each data point, the _silhouette_score_ is computed:\n",
    "\n",
    "> &nbsp;  \n",
    "> $ silhouette\\_score = \\left( p - q \\right) / max \\left( p, q \\right) $\n",
    "> <br><br>\n",
    "\n",
    "with:\n",
    "\n",
    "$ p $ ... mean distance to the points in the nearest cluster that the data point is not a part of  \n",
    "$ q $ ... mean intra-cluster distance to all the points in its own cluster.\n",
    "\n",
    "The value of the _silhouette_score_ lies between [-1, 1]. \n",
    "<br><br>\n",
    "\n",
    "\n",
    "A score closer to 1 indicates that the data point is very similar to other data points in the cluster, whereas a score closer to -1 indicates that the data point is not similar to the data points in its cluster. \n",
    "\n",
    "One way to think about it is if you get too many points with negative silhouette scores, then we may have too few or too many clusters in our data. We need to run the clustering algorithm again to find the optimal number of clusters.\n",
    "\n",
    "Let's see how to estimate the clustering performance using silhouette scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the data above (X, X_train, X_test)\n",
    "values = np.arange(2, 16)\n",
    "scores = []\n",
    "\n",
    "# Iterate through the defined range\n",
    "for k in values:\n",
    "    kM = KMeans(n_clusters=k, n_init=\"auto\")\n",
    "    kM.fit(X_train)\n",
    "    score = silhouette_score(X_train, kM.labels_, metric='euclidean', sample_size=len(X_train))\n",
    "    scores.append([k, score])\n",
    "\n",
    "# find best k and plot\n",
    "ss = np.array(scores)\n",
    "max_ss = max(ss[:, 1])\n",
    "best = [i for i, ss in enumerate(ss[:, 1]) if ss == max_ss][0]\n",
    "best_k, best_ss = ss[best, 0], ss[best, 1]\n",
    "plt.plot(ss[:, 1], ss[:, 0])\n",
    "plt.scatter(best_ss, best_k, c=\"white\", s=80, edgecolors=\"red\")\n",
    "plt.title(f\"Best silhouette score: {best_ss:.3f} for k={best_k:.0f}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This verifies our initial guess, that our data contains ___5___ clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner inertia\n",
    "\n",
    "Assuming the data points of a cluster should be \"close\" to the cluster centers (and the clusters should be \"far\" from each other, i.e. well-separated).\n",
    "\n",
    "This means that the __inner variance__ (= sum of squared distances of data points to cluster center) should be as small as possible. \n",
    "\n",
    "The total variance is returned as `inertia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total variance and visualize inertia\n",
    "total_variance = [KMeans(n_clusters=k, n_init=\"auto\").fit(X_train).inertia_ for k in values]\n",
    "\n",
    "plt.plot(total_variance)\n",
    "plt.title(f\"The gradient of total variance changes most at k={best_k:.0f}\")\n",
    "plt.scatter(3, total_variance[3], c=\"white\", s=80, edgecolors=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum number of clusters can be found where the inertia changes its gradient significantly (where the graph \"bends\").\n",
    "\n",
    "Computing the optimum number of clusters from the inertia requires some Python programming (that we leave to the student as an exercise)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_students_input.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><h5>Students task:</h5>Program:\n",
    "<ul>\n",
    "    <li>How can the optimal number of clusters can be derived from inertia?</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Chapter 20 (p 337ff) from \"Data Science from Scratch\"</li>\n",
    "<li>Scikit Learn on <a href=\"https://scikit-learn.org/stable/modules/clustering.html\">Clustering</a></li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propagation\n",
    "\n",
    "`Affinity propagation` does not require to define the numbers of clusters in advance.\n",
    "\n",
    "It is based on node similarities and _message passing_ between adjacent, similar nodes.\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/6_affinity_1.png\" width=\"400px\"><br><br>\n",
    "<img src=\"./img/6_affinity_2.png\" width=\"400px\"><br><br>\n",
    "<span style=\"font-size: 70%\">Source: <a href=\"http://www.ijeee.net/uploadfile/2014/0807/20140807114023665.pdf\">Map/Reduce Affinity Propagation Clustering Algorithm, Wei-Chih Hung, Chun-Yen Chu, and Yi-Leh Wu (2015)</a></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "from sklearn.metrics import silhouette_score, \\\n",
    "                            homogeneity_score, \\\n",
    "                            completeness_score, \\\n",
    "                            v_measure_score, \\\n",
    "                            adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the data above (X, X_train, X_test)\n",
    "\n",
    "# train model\n",
    "af = AffinityPropagation(preference=-100, random_state=1).fit(X_train)\n",
    "k = len(af.cluster_centers_)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "print(f\"Number of clusters: {k}\")\n",
    "print(f\"Cluster centers:\\n{af.cluster_centers_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "for c, col in zip(range(k), colors):\n",
    "    x = X_train[:, 0][af.labels_ == c]\n",
    "    y = X_train[:, 1][af.labels_ == c]\n",
    "    plt.scatter(af.cluster_centers_[c][0],\n",
    "                af.cluster_centers_[c][1],\n",
    "                s=80, color=col, edgecolors=\"#000000\")\n",
    "    plt.scatter(x, y, alpha=0.5, color=col)\n",
    "    for i in range(len(x)):\n",
    "        plt.plot([x[i], af.cluster_centers_[c][0]], \n",
    "                 [y[i], af.cluster_centers_[c][1]], \n",
    "                 alpha=0.5, color=col, linewidth=1.0)\n",
    "\n",
    "# plot test data\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], s=20, alpha=0.5, color=\"#666666\")\n",
    "plt.title(f\"Estimated number of clusters: {k}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Affinity` clustering can be used in recommender systems to cluster similar items (based on interest, preferences or other metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Scikit Learn on <a href=\"https://scikit-learn.org/stable/modules/clustering.html#affinity-propagation\">affinity propagation clustering</a></li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "Hierarchical cluster algorithms group data in trees (`dendrograms`). Two basic approaches distinguish the cluster algorithm:\n",
    "\n",
    "- __Agglomerative__ (bottom up): starts from the closest pair in the data, merging additional data to it\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"./img/6_Agglomerative_Clustering.png\" width=\"500px\">\n",
    "\n",
    "- __Divisive__ (top down): all data are member of the primary cluster that gets split recursively\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"./img/6_Divisive_Clustering.png\" width=\"500px\">\n",
    "\n",
    "<br><span style=\"font-size: 70%\">Source: <a href=\"https://www.datacamp.com/tutorial/introduction-hierarchical-clustering-python\">DataCamp: An Introduction to Hierarchical Clustering in Python</a></span>\n",
    "\n",
    "##### Measuring distance\n",
    "\n",
    "Hierarchical clusters measure the `distance` between pairs of cluster members.\n",
    "\n",
    "__Single linkage__:\n",
    "\n",
    "$ distance (C_i, C_j) = min \\left\\{ d (X_i, X_j) \\right\\} $ &nbsp; ... with $ i, j $ denoting Clusters\n",
    "\n",
    "<br>\n",
    "\n",
    "__Complete linkage__:\n",
    "\n",
    "$ distance (C_i, C_j) = max \\left\\{ d (X_i, X_j) \\right\\} $ \n",
    "\n",
    "<br>\n",
    "\n",
    "__Average linkage__:\n",
    "\n",
    "$ distance (C_i, C_j) = \\dfrac{ \\sum \\left\\{ d (X_i, X_j) \\right\\} }{n} $  &nbsp; ... with $ n $ total number of distances\n",
    "\n",
    "<br>\n",
    "\n",
    "`Scikit learn` supports agglomerative clustering."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>DataCamp on <a href=\"https://www.datacamp.com/tutorial/introduction-hierarchical-clustering-python\">hierarchical clustering</a></li>\n",
    "<li>Scikit Learn on <a href=\"https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering\">agglomerative clustering</a></li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, FeatureAgglomeration\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the data above (only X, as agglomerative clustering cannot predict unseen data - doesn't make sense)\n",
    "k = 5\n",
    "\n",
    "# train model\n",
    "agg = AgglomerativeClustering(n_clusters=k).fit(X)\n",
    "c_pred = agg.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "# DecisionBoundaryDisplay.from_estimator(agg, X, alpha=0.3, response_method=\"fit_predict\")\n",
    "for c, col in zip(range(k), colors):\n",
    "    x = X[:, 0][c_pred == c]\n",
    "    y = X[:, 1][c_pred == c]\n",
    "    plt.scatter(x, y, alpha=0.5)\n",
    "plt.title(\"Agglomeration cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Agglomerative clustering` assigns clusters a little different (see intersection of red / blue here and orange / blue in the affinity graph).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based spatial clustering of applications (DBSCAN)\n",
    "\n",
    "`DBSCAN`, given a set of points, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\n",
    "\n",
    "<img src=\"./img/6_DBSCAN.png\" width=\"400px\"><br><br>\n",
    "\n",
    "<span style=\"font-size: 70%\">In this diagram, minPts = 4. Point <b>A</b> and the other <b>red points</b> are <b>core points</b>, because the area surrounding these points in an &epsilon; radius contain at least 4 points (including the point itself). Because they are all reachable from one another, they form a single cluster.<br> Points <b>B</b> and <b>C</b> are not core points, but are <b>reachable from A</b> (via other core points) and thus belong to the cluster as well. <br>Point <b>N</b> is a <b>noise point</b> that is neither a core point nor directly-reachable.<br>Source: <a href=\"https://en.wikipedia.org/wiki/DBSCAN\">Wikipedia</a></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "__DBSCAN pseudo code__:\n",
    "\n",
    "<pre>DBSCAN(DB, distFunc, eps, minPts) {\n",
    "    C := 0                                                  /* Cluster counter */\n",
    "    for each point P in database DB {\n",
    "        if label(P) ≠ undefined then continue               /* Previously processed in inner loop */\n",
    "        Neighbors N := RangeQuery(DB, distFunc, P, eps)     /* Find neighbors */\n",
    "        if |N| < minPts then {                              /* Density check */\n",
    "            label(P) := Noise                               /* Label as Noise */\n",
    "            continue\n",
    "        }\n",
    "        C := C + 1                                          /* next cluster label */\n",
    "        label(P) := C                                       /* Label initial point */\n",
    "        SeedSet S := N \\ {P}                                /* Neighbors to expand */\n",
    "        for each point Q in S {                             /* Process every seed point Q */\n",
    "            if label(Q) = Noise then label(Q) := C          /* Change Noise to border point */\n",
    "            if label(Q) ≠ undefined then continue           /* Previously processed (e.g., border point) */\n",
    "            label(Q) := C                                   /* Label neighbor */\n",
    "            Neighbors N := RangeQuery(DB, distFunc, Q, eps) /* Find neighbors */\n",
    "            if |N| ≥ minPts then {                          /* Density check (if Q is a core point) */\n",
    "                S := S ∪ N                                  /* Add new neighbors to seed set */\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "RangeQuery(DB, distFunc, Q, eps) {\n",
    "    Neighbors N := empty list\n",
    "    for each point P in database DB {                      /* Scan all points in the database */\n",
    "        if distFunc(Q, P) ≤ eps then {                     /* Compute distance and check epsilon */\n",
    "            N := N ∪ {P}                                   /* Add to result */\n",
    "        }\n",
    "    }\n",
    "    return N\n",
    "}</pre>\n",
    "\n",
    "<br>\n",
    "\n",
    "`DBSCAN` can find non-linearly separable clusters. \n",
    "\n",
    "<img src=\"./img/6_DBSCAN_2.png\" width=\"250px\"><br><br>\n",
    "\n",
    "<span style=\"font-size: 70%\">This dataset cannot be adequately clustered with k-means or Gaussian Mixture EM clustering.<br>Source: <a href=\"https://en.wikipedia.org/wiki/DBSCAN\">Wikipedia</a></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the data above\n",
    "\n",
    "# train model\n",
    "dbs = DBSCAN().fit(X)\n",
    "labels = dbs.labels_\n",
    "k = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(f\"Estimated number of clusters: {k}\")\n",
    "print(f\"Estimated number of noise points: {n_noise_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "for c, col in zip(range(k), colors):\n",
    "    x = X[:, 0][labels == c]\n",
    "    y = X[:, 1][labels == c]\n",
    "    plt.scatter(x, y, s=20)\n",
    "plt.scatter(X[:, 0][labels == -1], X[:, 1][labels == -1], s=20, color=\"#666666\", alpha=0.5)\n",
    "plt.title(f\"DBSCAN finds k={k} clusters and {n_noise_} noise points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 128px\">&#9749;</span> Coffee break!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
