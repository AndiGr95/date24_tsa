{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Classifier\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/5_SVM_margin.png\" width=\"400px\">\n",
    "\n",
    "<span style=\"font-size: 70%\">Source: [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)</span>\n",
    "<br><br>\n",
    "\n",
    "`SVM` maps training examples to points in space so as to __maximise__ the width of __the gap between the two categories__. \n",
    "\n",
    "New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Some theory\n",
    "\n",
    "Given a training set: &nbsp; $ (X, y) $\n",
    "\n",
    "we can define a hyperplane:\n",
    "\n",
    "$ w^T x - b = 0 $ &nbsp;&nbsp; with $ w $ ... normal vector of the hyperplane &nbsp;&nbsp;  and $ \\frac{b}{\\left\\| w \\right\\|} $ ... offset of the hyperplane from the origin\n",
    "\n",
    "##### Hard margin\n",
    "\n",
    "If the training data is linealy seperable, we can find two additional hyperplanes that clearly select decision boundaries and maximize the distance between these two hyperplanes.\n",
    "\n",
    "$ w^T x - b \\ge 1 $ &nbsp; for class 0 (blue dots in the figure above)\n",
    "\n",
    "$ w^T x - b \\le -1 $ &nbsp; for class 1 (green dots in the figure above)\n",
    "\n",
    "The distance between these two hyperplanes is $ \\frac{2}{\\left\\| w \\right\\|} $.  \n",
    "To maximize the distance, we need to minimize $ \\left\\| w \\right\\| $.\n",
    "\n",
    "<br>\n",
    "This problem is solved using the signum function.\n",
    "\n",
    "<img src=\"./img/5_Signum_function.png\" width=\"300px\"><br><br>\n",
    "\n",
    "##### Soft margin\n",
    "\n",
    "If data is not linearly seperable (some points fall into the margin or even in the area of the other class), we might use:\n",
    "\n",
    "$ \\max \\left( 0, 1 - y_i \\left( w^T x_i - b \\right) \\right) $ &nbsp; ... Hinge loss\n",
    "\n",
    "<img src=\"./img/5_Hinge_loss.png\" width=\"300px\"><br><br>\n",
    "\n",
    "The goal is to optimize (minimize)\n",
    "\n",
    "$ \\lambda \\left\\| w \\right\\|^2 + \\left[ \\frac{1}{n} \\sum_{i=1}^n \\max \\left( 0, 1 - y_i \\left( w^T x_i - b \\right) \\right) \\right] $ &nbsp;&nbsp; with $ \\lambda \\gt 0 $ ... tradeoff between margin size and classification errors.\n",
    "\n",
    "##### Nonlinear classification\n",
    "\n",
    "Nonlinear data can be classified using [`kernel tricks`](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick) - projections of data into higher dimensional spaces for separation.\n",
    "\n",
    "<img src=\"./img/5_Kernel_trick_idea.png\" width=\"600px\">\n",
    "\n",
    "<span style=\"font-size: 70%\">The data shown cannot be separated by drawing a straight line (<b>left</b>). Linear classification is not possible in this case.<br>By projecting the data into 3D space, data becomes separable along a flat hyperplane (<b>right</b>).<br>Source: [Wikipedia](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)</span><br><br>\n",
    "\n",
    "SVM with kernel &nbsp; $ \\phi \\left( a, b \\right) = \\left( a, b, a^2 + b^2 \\right) $\n",
    "\n",
    "$ \\to k \\left( x, y \\right) = x \\cdot y + \\left\\| x \\right\\|^2 \\left\\| y \\right\\|^2 $\n",
    "\n",
    "The training points are mapped to a 3-dimensional space where a separating hyperplane can be easily found.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Popular kernels:__\n",
    "\n",
    "- Polynomial: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ k \\left( x, y \\right) = \\left( x_i, y_j \\right)^d $ &nbsp;&nbsp; with $ d = 1 $ ... linear kernel\n",
    "- Gaussian RBF (radial basis function): &nbsp; $ k \\left( x, y \\right) = \\exp \\left( - \\gamma \\left\\| x_i - x_j \\right\\|^2 \\right) $\n",
    "- Sigmoid: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ k \\left( x, y \\right) = \\tanh \\left( \\kappa x_i \\cdot x_j + c \\right) $\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_reference.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><u>Further reading:</u>\n",
    "<ul>\n",
    "<li>Read about <a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\">Support Vector Machine</a> in Wikipedia,</li>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/svm.html\">Support Vector Machines (SVM)</a> in Scikit learn</li>\n",
    "<li>12. Support Vector Machines (SVM) - theory and practice in <br>\"<a href=\"./material/The Complete Hands-On Machine Learning Crash Course, M.Peixeiro (2019).pdf\">The Complete Hands-On Machine Learning Crash Course</a>\" p44 ff</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS Classification using SVM\n",
    "\n",
    "Let's apply our knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model generation\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model quality\n",
    "\n",
    "y_pred=svm.predict(X_test)\n",
    "\n",
    "# generate a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# visualize the confusion matrix\n",
    "ax = plt.axes()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, cmap=\"Greens\", ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy:')\n",
    "print(f' Train: {accuracy_score(y_train, svm.predict(X_train))*100:.2f} %')\n",
    "print(f' Test:  {accuracy_score(y_test, y_pred)*100:.2f} %')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for display of decission boundaries\n",
    "from itertools import product\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# create a dataframe for statistic evaluation\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "df = pd.DataFrame(iris.data, columns=feature_columns)\n",
    "df['Species'] = pd.Series(iris.target_names[iris.target])\n",
    "\n",
    "# all combinations to compare\n",
    "X_cols_combo = [[\"SepalLengthCm\", \"SepalWidthCm\"], [\"SepalLengthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalLengthCm\", \"PetalWidthCm\"], [\"SepalWidthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalWidthCm\", \"PetalWidthCm\"], [\"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# comparing datasets\n",
    "X1 = df[X_cols_combo[0]].to_numpy()\n",
    "X2 = df[X_cols_combo[1]].to_numpy()\n",
    "X3 = df[X_cols_combo[2]].to_numpy()\n",
    "X4 = df[X_cols_combo[3]].to_numpy()\n",
    "X5 = df[X_cols_combo[4]].to_numpy()\n",
    "X6 = df[X_cols_combo[5]].to_numpy()\n",
    "y = iris.target\n",
    "\n",
    "# generating a forrest of classifiers\n",
    "clf_1 = SVC()\n",
    "clf_2 = SVC()\n",
    "clf_3 = SVC()\n",
    "clf_4 = SVC()\n",
    "clf_5 = SVC()\n",
    "clf_6 = SVC()\n",
    "\n",
    "# make things iterable\n",
    "clf_all = [clf_1, clf_2, clf_3, clf_4, clf_5, clf_6]\n",
    "X_sets = [X1, X2, X3, X4, X5, X6]\n",
    "\n",
    "# helper to generate the image position\n",
    "img_pos = [pos_t for pos_t in product([0, 1, 2], [0, 1])]\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "for i, classifier in enumerate(clf_all):\n",
    "    classifier.fit(X_sets[i], y)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, X_sets[i], alpha=0.4, ax=ax[img_pos[i][0], img_pos[i][1]], response_method=\"predict\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].scatter(X_sets[i][:, 0], X_sets[i][:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].set_title(f\"{X_cols_combo[i][0][:-2]} vs. {X_cols_combo[i][1][:-2]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing kernels\n",
    "\n",
    "It is interesting to see how different kernels separate and classify this data.\n",
    "\n",
    "We use Petal length vs. Petal Width in this example but you can experiment with different combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only needed for display of decission boundaries\n",
    "\n",
    "feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "df = pd.DataFrame(iris.data, columns=feature_columns)\n",
    "df['Species'] = pd.Series(iris.target_names[iris.target])\n",
    "\n",
    "# all combinations to compare\n",
    "X_cols_combo = [[\"SepalLengthCm\", \"SepalWidthCm\"], [\"SepalLengthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalLengthCm\", \"PetalWidthCm\"], [\"SepalWidthCm\", \"PetalLengthCm\"],\n",
    "                [\"SepalWidthCm\", \"PetalWidthCm\"], [\"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "\n",
    "# comparing datasets\n",
    "X = df[X_cols_combo[5]].to_numpy()                              # change the X_cols_combo index to experiment with different combinations\n",
    "y = iris.target\n",
    "\n",
    "# generating a forrest of classifiers\n",
    "clf_1 = SVC(kernel=kernels[0])\n",
    "clf_2 = SVC(kernel=kernels[1])\n",
    "clf_3 = SVC(kernel=kernels[2])\n",
    "clf_4 = SVC(kernel=kernels[3], gamma=\"auto\")\n",
    "\n",
    "# make things iterable\n",
    "clf_all = [clf_1, clf_2, clf_3, clf_4]\n",
    "\n",
    "# helper to generate the image position\n",
    "img_pos = [pos_t for pos_t in product([0, 1], [0, 1])]\n",
    "\n",
    "f, ax = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "for i, classifier in enumerate(clf_all):\n",
    "    classifier.fit(X6, y)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        classifier, X, alpha=0.8, ax=ax[img_pos[i][0], img_pos[i][1]], response_method=\"predict\", cmap=plt.cm.RdBu)\n",
    "    ax[img_pos[i][0], img_pos[i][1]].scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    ax[img_pos[i][0], img_pos[i][1]].set_title(f\"{kernels[i]} kernel\")\n",
    "    \n",
    "plt.suptitle(\"SVC comparison: PetalLength vs. PetalWidth\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that the `sigmoid` (or $tanh$) kernel does not separate `Iris virginica` and `Iris versicolor` at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"border-style: none\"><img src=\"./img/0_students_input.png\" height=\"100px\"></td>\n",
    "<td style=\"border-style: none\">&nbsp;&nbsp;</td>\n",
    "<td style=\"border-style: none; vertical-align: middle\"><h5>Students task: Discuss</h5>\n",
    "<ul>\n",
    "    <li>Why does the sigmoid kernel not differentiate all classes (possible reasons)?</li>\n",
    "    <li>How could we correctly classify all three classes using the sigmoid kernel?</li>\n",
    "</ul>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 128px\">&#9749;</span> Coffee break!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
